{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent build with langchain, memory, react_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Setup of Environment and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Setup\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load local env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.environ['OPENAI_API_KEY'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Search Tool and its configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent specific imports\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "search = TavilySearchResults(\n",
    "    max_results=5,\n",
    "    search_depth=\"advanced\",\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    include_images=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Agent that can run action\n",
    "- Use Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "# Define agent executor by providing the base model and search tool separately as input\n",
    "# remember model with tools that is defined above is NOT used here\n",
    "# react agent can bind the search toool\n",
    "agent_executor = create_react_agent(model, [search])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the agent with example that does not require LLM to use Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent with example that does not require LLM to use Search Tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "response = agent_executor.invoke({\"messages\": [HumanMessage(content=\"Hello\")]})\n",
    "print(f\"response[messages] : {response[\"messages\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the agent with example that requires LLM to use Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent with example that requires LLM to use Search Tool\n",
    "response = agent_executor.invoke({\"messages\": [HumanMessage(content=\"What is the weather like in Sparks, Maryland today?\")]})\n",
    "print(f\"response[messages] : {response[\"messages\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stream Responses instead of all at once print\n",
    "- In case the agent has to execute multiple steps a better approach is to stream output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = 1\n",
    "for msg_chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the weather like in Sparks, Maryland today?\")]}\n",
    "):\n",
    "    print(\"========>\")\n",
    "    print(f\"message segment {segment} : {msg_chunk}\")\n",
    "    segment += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Memory to make the agent stateful\n",
    "- Use langgraph memory : This checkpoint saver stores checkpoints in memory using a `defaultdict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "agent_executor = create_react_agent(model, [search], checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test by invoking agent\n",
    "\n",
    "# Configure a session id to track thread\n",
    "config = {\"configurable\": {\"thread_id\": \"utkal-11-03-24\"}}\n",
    "segment = 1\n",
    "for msg_chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the weather like in Sparks, Maryland today?\")]},\n",
    "    config=config\n",
    "):\n",
    "    print(\"========>\")\n",
    "    print(f\"message segment {segment} : {msg_chunk}\")\n",
    "    segment += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory by asking a follow up question\n",
    "for msg_chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"How is it going to be this whole week?\")]},\n",
    "    config=config\n",
    "\n",
    "):\n",
    "    print(\"========>\")\n",
    "    print(f\"message segment {segment} : {msg_chunk}\")\n",
    "    segment += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
