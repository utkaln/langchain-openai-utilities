{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Langchain using Open AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import OpenAI \n",
    "- First install Open AI using `pip install openai`\n",
    "- install env utilities `pip install python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import datetime\n",
    "# Load local env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "# Read openai api key from local environment\n",
    "llm_model = 'gpt-3.5-turbo'\n",
    "temperature=0.2\n",
    "open_ai = OpenAI(\n",
    "  organization=os.environ['OPENAI_ORG'],\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    ")\n",
    "\n",
    "# Input Prompt\n",
    "# test with a prompt\n",
    "user_msg = \"I have not had french fries in a long time, may we have some please !\"\n",
    "style = \"Assume role of a Gen Alpha kid persona talking in an informal and exaggerated persona to his/her friend\"\n",
    "prompt = f\"\"\"Rephrase the following text that is delimited by triple backticks in {style}. text: ```{user_msg}```\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open AI without Langchain\n",
    "- Without langchain following block is how open ai calls are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have not indulged in the delectable delight of French fries for quite some time. Would you kindly accompany me in partaking in this savory treat?"
     ]
    }
   ],
   "source": [
    "# define functiont to complete\n",
    "def get_completion_gpt(prompt,model=llm_model):\n",
    "    messages = [{\"role\":\"user\",\"content\":prompt}]\n",
    "    response = open_ai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        stream=True,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "response = get_completion_gpt(prompt)\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open AI with LangChain and Parser\n",
    "- Install Langchain first \n",
    "- Use LangSmith for Observability [Official Document](https://python.langchain.com/docs/tutorials/llm_chain/)\n",
    "- Signup for LangSmith API key [How To...](https://smith.langchain.com/onboarding?organizationId=a1b11d3c-e3c3-4439-9ebc-a077a7c0de67&step=1)\n",
    "- Trace via LangSmith [Link](https://smith.langchain.com/o/a1b11d3c-e3c3-4439-9ebc-a077a7c0de67/projects/p/9deedd89-2b39-4469-afaa-fe78f12f0bf2?timeModel=%7B%22duration%22%3A%227d%22%7D&tab=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"O-M-G, dude, I am literally craving some french fries like you wouldn't believe! It's been FOREVER since I've had those crispy, salty, golden sticks of deliciousness. Can we pleeease go get some ASAP? My taste buds are screaming for some fry action!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install -U langchain langchain-openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=llm_model)\n",
    "\n",
    "# import system prompt and user prompt from langchain\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=f\"\"\"Translate the following text into a persona that is {style}\"\"\"),\n",
    "    HumanMessage(content=user_msg)\n",
    "]\n",
    "\n",
    "# import langchain parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# parser.invoke(llm.invoke(messages))\n",
    "# More modern way of invoking the response and parsing is by using |\n",
    "chain = llm | parser\n",
    "chain.invoke(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
