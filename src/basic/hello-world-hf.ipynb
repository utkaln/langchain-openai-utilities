{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Langchain using Hugging Face AI - Llama 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Hugging Face Interface client \n",
    "- First install Hugging face interface client using `pip install huggingface_hub`\n",
    "- install env utilities `pip install python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import datetime\n",
    "# Load local env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "# Read openai api key from local environment\n",
    "llm_model = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "temperature=0.2\n",
    "hf_ai = InferenceClient(token=os.environ['HUGGINGFACEHUB_API_TOKEN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF without Langchain\n",
    "- Without langchain following block is how open ai calls are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the rephrased text:\n",
      "\n",
      "```\n",
      "Ohmygosh, dude! I'm literally dying over here! It's been, like, forever since I've had donuts! Can we PLEEEEEEase get some? I'm talking about the kind with the sprinkles and the frosting, you know? The ones that are, like, SOOOO good?! I'm literally craving them right now, and I don't know how much longer I can hold out! So"
     ]
    }
   ],
   "source": [
    "# define functiont to complete\n",
    "def get_completion_gpt(prompt,model=llm_model):\n",
    "    messages = [{\"role\":\"user\",\"content\":prompt}]\n",
    "    response = hf_ai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        stream=True,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# test with a prompt\n",
    "user_msg = \"I have not had donuts in a long time, may we have some please !\"\n",
    "style = \"Assume role of a Gen Alpha kid persona talking very casually to his/her friend in an elaborative expression\"\n",
    "prompt = f\"\"\"Rephrase the following text that is delimited by triple backticks in {style}. text: ```{user_msg}```\"\"\"\n",
    "\n",
    "response = get_completion_gpt(prompt)\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF AI with LangChain\n",
    "- Install Langchain first \n",
    "- Use LangSmith for Observability [Official Document](https://python.langchain.com/docs/tutorials/llm_chain/)\n",
    "- Signup for LangSmith API key [How To...](https://smith.langchain.com/onboarding?organizationId=a1b11d3c-e3c3-4439-9ebc-a077a7c0de67&step=1)\n",
    "- Trace via LangSmith [Link](https://smith.langchain.com/o/a1b11d3c-e3c3-4439-9ebc-a077a7c0de67/projects/p/9deedd89-2b39-4469-afaa-fe78f12f0bf2?timeModel=%7B%22duration%22%3A%227d%22%7D&tab=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ugh, my class is so annoying! The kids are always being wild and crazy.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install -U langchain langchain-openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=llm_model)\n",
    "\n",
    "# import system prompt and user prompt from langchain\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=f\"\"\"Translate the following text into a persona that is {style}\"\"\"),\n",
    "    HumanMessage(content=user_msg)\n",
    "]\n",
    "\n",
    "# import langchain parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# parser.invoke(llm.invoke(messages))\n",
    "# More modern way of invoking the response and parsing is by using |\n",
    "chain = llm | parser\n",
    "chain.invoke(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
